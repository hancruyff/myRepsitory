{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 몬테카를로 예측 알고리즘\n",
    "def mc_prediction(pi, env, gamma=1.0, # 정책, 환경, 감마(할인율)\n",
    "    init_alpha=0.5, min_alpha=0.01, alpha_decay_ratio=0.5, # 알파(학습계수): 초기값, 최종값, 감가비율\n",
    "    n_episodes=500, max_steps=200, # 에피소드 수, 에피소드의 최대 스텝 수\n",
    "    first_visit=True): # 첫 방문(True), 모든 방문(False) 여부\n",
    "    nS = env.observation_space.n # 상태 수\n",
    "# 할인율(감마)을 한 번에 계산, [r0, r1, r2, ....., rmax_steps-1]\n",
    "# 파라미터: 시작값, 최종값, 샘플수, 로그 베이스, 최종값 포함 여부\n",
    "    discounts = np.logspace(0, max_steps, num=max_steps, base=gamma, endpoint=False)\n",
    "# 에피소드마다 지수적 감가되는 알파(학습계수) 값을 한 번에 계산\n",
    "    alphas = decay_schedule(init_alpha, min_alpha, alpha_decay_ratio, n_episodes)\n",
    "    V = np.zeros(nS, dtype=np.float64) # 상태-가치함수 초기화\n",
    "# 오프라인 분석(에피소드 당 가치함수, 타겟(리턴) 값)을 위한 변수 초기화\n",
    "    V_track = np.zeros((n_episodes, nS), dtype=np.float64)\n",
    "    targets = {state:[] for state in range(nS)}\n",
    "    # 에피소드 당 반복, tqdm 라이브러리 사용하여 진행률(progress bar) 표시\n",
    "# 잔상 표시하지 않음(leave=False), pip install tqdm으로 설치\n",
    "    for e in tqdm(range(n_episodes), leave=False):\n",
    "# 전체 경로의 경험 튜플을 한 번에 생성\n",
    "        trajectory = generate_trajectory(pi, env, max_steps)\n",
    "        visited = np.zeros(nS, dtype=np.bool) # 각 상태의 방문 여부 벡터 초기화\n",
    "# 에피소드의 각 상태 당 반복\n",
    "        for t, (state, _, reward, _, _) in enumerate(trajectory):\n",
    "            if visited[state] and first_visit: # 이미 방문한 상태이고 FVMC 이면 다음 상태로 이동\n",
    "                continue\n",
    "            visited[state] = True # 방문 상태로 표시\n",
    "            n_steps = len(trajectory[t:]) # 최종 상태까지의 스텝 수\n",
    "# 리턴 값 계산\n",
    "            G = np.sum(discounts[:n_steps] * trajectory[t:, 2])\n",
    "            targets[state].append(G)\n",
    "# MC 에러 계산\n",
    "            mc_error = G - V[state]\n",
    "# 상태 갱신\n",
    "            V[state] = V[state] + alphas[e] * mc_error\n",
    "        V_track[e] = V\n",
    "    return V.copy(), V_track, targets # 상태-가치함수 리턴(deep copy)\n",
    "## 에피소드마다 지수적 감가되는 알파(학습계수) 값 한 번에 계산\n",
    "def decay_schedule(init_value, min_value, decay_ratio, max_steps, log_start=-2, log_base=10):\n",
    "    decay_steps = int(max_steps * decay_ratio) # 감가 비율 사용하여 감가될 스텝(횟수) 게산\n",
    "    rem_steps = max_steps - decay_steps # 나머지 스텝\n",
    "# [10-2, .......... 100]의 역순([::-1])\n",
    "    values = np.logspace(log_start, 0, decay_steps, base=log_base, endpoint=True)[::-1]\n",
    "# 0과 1 사이의 값으로 정규화\n",
    "    values = (values - values.min()) / (values.max() - values.min())\n",
    "# init_value와 min_value 사이 값으로 조정\n",
    "    values = (init_value - min_value) * values + min_value\n",
    "# 나머지 스텝은 마지막 min_value로 패딩\n",
    "    values = np.pad(values, (0, rem_steps), 'edge')\n",
    "    return values\n",
    "## 에피소드의 전체 경로의 경험 튜플을 한 번에 생성\n",
    "def generate_trajectory(pi, env, max_steps=200):\n",
    "    done, trajectory = False, []\n",
    "    while not done:\n",
    "        state = env.reset()\n",
    "        state = state[0]\n",
    "        for t in count():\n",
    "            action = pi(state) # 해당 정책의 행동 선택\n",
    "            next_state, reward, done, _, _ = env.step(action) # 행동 수행 후 다음 상태, 보상 수집\n",
    "# 경험 튜플 저장\n",
    "            experience = (state, action, reward, next_state, done)\n",
    "            trajectory.append(experience)\n",
    "            if done: # 에피소드 최종 상태 도달 시 종료\n",
    "                break\n",
    "            if t >= max_steps - 1: # 최대 스텝 수 초과 시 초기화 후 반복\n",
    "                trajectory = []\n",
    "                break\n",
    "        state = next_state\n",
    "    return np.array(trajectory, np.object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tqdm import *\n",
    "from itertools import cycle, count\n",
    "import gym\n",
    "from RLHelper import * # 헬퍼 모듈\n",
    "## 몬테카를로 예측 알고리즘...............................\n",
    "## 프로즌 레이크 환경 생성\n",
    "env = gym.make('FrozenLake-v1')\n",
    "# 파라미터 초기화\n",
    "init_state = env.reset()\n",
    "goal_state = 15\n",
    "gamma = 0.99\n",
    "n_episodes = 2500\n",
    "P = env.env.P # 상태\n",
    "## 샘플 정책 생성\n",
    "LEFT, DOWN, RIGHT, UP = range(4)\n",
    "pi = lambda s: {\n",
    "0:LEFT, 1:UP, 2:UP, 3:UP,\n",
    "4:LEFT, 5:LEFT, 6:LEFT, 7:LEFT,\n",
    "8:UP, 9:DOWN, 10:LEFT, 11:LEFT,\n",
    "12:LEFT, 13:RIGHT, 14:DOWN, 15:LEFT\n",
    "}[s]\n",
    "## 참 상태-가치함수 계산: 동적 계획법의 정책 평가\n",
    "# 동적 계획법의 정책 평가 수행\n",
    "V_true = policy_evaluation(pi, P, gamma=gamma)\n",
    "# 가치함수, 정책, 성공 확률 출력\n",
    "print(\">> 참 상태-가치함수: 동적 계획법 정책 평가\")\n",
    "print_state_value_function(V_true, P)\n",
    "print()\n",
    "print_policy(pi, P)\n",
    "print('성공확률 {:.2f}%. 평균리턴값 {:.4f}.'.format(\n",
    "probability_success(env, pi, goal_state=goal_state),\n",
    "mean_return(env, gamma, pi)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
